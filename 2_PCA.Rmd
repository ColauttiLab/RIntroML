---
title: "Principal Components Analysis"
output: html_document
---

# Overview

Principal Components Analysis (PCA) is an example of an unsupervised machine learning model. However, PCA has long been used by ecologists and evolutionary biologists as a diminsion reduction tool and to look for structure in biological data. PCA also forms the basis for more discriminant analysis -- a form of supervised machine learning.

PCA is one type of multivariate analysis and related methods are commonly used to analyze all kinds of biological data including plant and animal communities, microbiomes, gene expression, metabolomics & proteomics, and climate. 

In this tutorial, we revisit the same data on *Lythrum salicaria* that we used for selection analysis in the [GAM Tutorial](https://colauttilab.github.io/RIntroStats/9_GAM.html).

# Setup

Load the usual plotting and data management functions:

```{r}
library(ggplot2) # plotting library
library(dplyr) # data management

source("http://bit.ly/theme_pub") # Set custom plotting theme
theme_set(theme_pub())
```

and the fitness data:

```{r}
LythrumDat<-read.csv("https://colauttilab.github.io/Data/ColauttiBarrett2013Data.csv",header=T)
```

# Dimensionality

Recall the structure of the data:

```{r}
str(LythrumDat)
```

We have the same traits measured in multiple years, in particular:

  * **Flwr** -- Days to first flower
  * **FVeg** -- Plant size (vegetative height) at first flower

Each trait was measured over 4 years, from 2007 through 2010. We can think of each trait as 'multivariate' or 'mutli-dimensional' with $D = 4$ dimensions.

The **Principal Components** of a PCA are new vectors that are calculated as linear combinations of other vectors. 

Think of a linear model where we try to predict a single response variable $Y$ by adding together coefficients of N individual predictors $X_1 ... X_N$. PCA is similar, except that instead of predicting a measured variable $Y$, we are trying to redefine N new component axes $PC_1 ... PC_N$ that are:

  1. Linear combinations of the input features $X_1 ... X_N$ ('predictors' in LM lingo)
  2. Uncorrelated with each other -- the correlation coefficient between any two PC axes should be ~ 0.

One common goal of PCA is to reduce the **dimensionality** of our data, by redefining many **correlated** predictors into one or a few **uncorrelated** principal component axes. These PC axes are themselves vectors of data representing different combinations of the input features.
  
In the GAM tutorial, we averaged across years in order to reduce the dimensionality from 4 to 1 for each of the key measurements (Flwr & FVeg). One problem with that approach is that it doesn't account for measurement error in each year.

# Correlated Traits

PCA works best when there is colinearity among the predictors. We can see this by looking at the pairwise correlations. To do this, let's first make two separate datasets, one for each of the main measurements. We can use the `select()` function from `dplyr` and specify every single column. However if we look at the help `?select` we can see a short-cut since the column names all have the same prefix:

```{r}
Flwr<-LythrumDat %>%
  select(starts_with("Flwr"))
FVeg<-LythrumDat %>%
  select(starts_with("FVeg"))
head(Flwr)
head(FVeg)
```

We can calculate a correlation matrix using `cor()`, but:

```{r, error=T}
cor(Flwr)
```

We just get NAs because we have missing data. If we look at the help `?cor` we can see there is a `use=` parameter with different options for excluding NA. In this case, we'll do each pairwise correlation and exclude only missing values in one or both of each pair.

```{r}
cor(Flwr,use="pairwise.complete.obs")
```

We can see some pretty high correlation coefficients. We can square these to get R-squared values, which we can interpret as how much variation in one metric is explained by the other. We might also want to round to make the values easier to compare

```{r}
round(cor(Flwr,use="pairwise.complete.obs")^2,3)
round(cor(FVeg,use="pairwise.complete.obs")^2,3)
```

Quite a bit of variation year-to-year but overall we can predict 35-75% of the variation. 

Visualizing these correlations is a bit tricky. One way to do this would be to convert the data from 'wide' to 'long' format using `pivot_longer` from the `tidyverse` package, and then use facets with ggplot. We would want to do this to look for outliers and nonlinear relationships among variables, but for the sake of time we'll skip those steps here.

# PCA

Running PCA in R is very straight-forward. 

## Missing Data 

One major limitation of PCA is that it can't handle missing data.

The only problem is that PCA doesn't work with missing data. To run this we can remove any rows with any missing observations. If we have observations of growth for a plant in 2007, 2008, and 2010 but we are missing 2009 then we have to remove the entire row! 

## Imputation

We only have a few hundred rows of data so we want to avoid removing any observations that we have. Rather than delete entire rows, we can try to 'impute' the missing data. 

One simple method is just to replace the NA with the mean, median, or mode of the column of data. 

A more complicated method is to use some kind of predictive model. For example, a linear model:

```{r}
Mod<-lm(Flwr09 ~ Flwr07 + Flwr08 + Flwr10, data=Flwr)
summary(Mod)
```

Here we can see that we can predict about 72% of the variation in 2009 by knowing the other 3 years. We can replace the NA with the prediction from the model. There are more complicated 'imputation' models that actually use machine learning algorithms to predict missing values. If we run everything in R, we can try different transformations to see how they affect the final visualizations and statistical analyses.

We'll keep it simple for now and replace NA with the column average.

```{r}
Flwr<-Flwr %>%
  mutate(Flwr07 = ifelse(is.na(Flwr07),mean(Flwr07,na.rm=T),Flwr07),
         Flwr08 = ifelse(is.na(Flwr08),mean(Flwr08,na.rm=T),Flwr08),
         Flwr09 = ifelse(is.na(Flwr09),mean(Flwr09,na.rm=T),Flwr09),
         Flwr10 = ifelse(is.na(Flwr10),mean(Flwr10,na.rm=T),Flwr10))

FVeg<-FVeg %>%
  mutate(FVeg07 = ifelse(is.na(FVeg07),mean(FVeg07,na.rm=T),FVeg07),
         FVeg08 = ifelse(is.na(FVeg08),mean(FVeg08,na.rm=T),FVeg08),
         FVeg09 = ifelse(is.na(FVeg09),mean(FVeg09,na.rm=T),FVeg09),
         FVeg10 = ifelse(is.na(FVeg10),mean(FVeg10,na.rm=T),FVeg10))
```

Now we are ready to run the PCA

# Princomp

The `princomp` function in base R is all we need for a principal components analysis.

## Scaling

In most cases, we would want to scale our predictors, as described in the [Intro to ML Tutorial](https://colauttilab.github.io/RIntroML/index.html#Scaling).

but we can also use the `cor=T` parameter to calculate principal components from the correlation matrix, rather than the covariance matrix. This is equivalent to standardizing to z-scores.

```{r}
FlwrPCA<-princomp(Flwr, cor=T)
```

Looking at the structure of our PCA:

```{r}
str(FlwrPCA)
```

We can see that the output is a list object, with some important components that we'll explore here.

# Principal Components

Here's where Principal Components Analysis gets its name: Every PCA analysis will return a number of principal component vectors (aka 'axes') equal to the number of input features. In this case, we have four columns of Flwr data from 2007 through 2010. This results in four PC axes (Comp.1 to Comp.4), which are `scores` in the output of a `princomp` list object.  

```{r}
head(FlwrPCA$scores)
```

A key characteristic of PC axes is that they are **uncorrelated** with each other. 

```{r}
round(cor(FlwrPCA$scores),3)
```

Here we see the correlation coefficients of the PC axes, rounded to 3 decimal places. Note that the diagonal is 1 representing the correlation of each PC axis with itself. The off-diagonals are zero, meaning that the correlation coefficient is < 0.001.

Compare this matrix to the correlation matrix for the original `Flwr` data, shown above. We have redefined linear combinations of the original data to take the original 4 correlated features and redefine them as 4 uncorrelated features.

In other words, we have rescaled four **correlated** measurements into four **uncorrelated** PC axes.

# Eigenvalues

Each of the four PC axes has an eigenvalue, which we can see in the summary:

```{r}
summary(FlwrPCA)
```

The first row shows the `Standard Deviation` for each PC axis This is literally just the standard deviation of the values of each PC:

```{r}
round(sd(FlwrPCA$scores[,1]),2)
```

Compare this value to the `Standard deviation` for Comp.1 above.


Squaring the standard deviation gives the variance, which is also known as the PC eigenvalue:

```{r}
round(sd(FlwrPCA$scores[,1])^2,2)
```

The eigenvalue of a PC axis is just the amount of variation (variance) in the observed data that can be explained by the PC axis. Notice above that the `sd` declines from PC1 to PC4. This is always the case: PCs are sorted by their eigenvalues, from highest to lowest.

The second row shows the `Proportion of variance`, which is how much of the total variance (= sum of all eigenvalues) is represented by each Principal Component axis.

The third row is just the `Cumulative Variance`, calculated by summing the variances. So the first column is the same, the second column is the first + second, and so on. 

If we are using PCA for dimesion reduction, we can use the **Proportion of Variance** explained by each PC to help us decide how many PCs to keep for downstream analysis. For example, recall how we averaged `Flwr` across years in the GAM tutorial to collapse 4 columns of data into 1. We can do something similar with PCA, but let the data tell us how many PCs we should keep.

A common method for this is to just plot the variance across eigenvectors. 

# Scree Plot

Looking back at the list items for FlwrPCA above:

```{r}
qplot(x=c(1:4),y=FlwrPCA$sdev^2) +
  geom_line() +
  xlab("Component") +
  ylab("Eigenvalue")
```

The x-axis is the PC number, ranked by eigenvalue. The y-axis is the eigenvalue, representing the amount of variation explained.

This is called a **Scree Plot**. It can help us choose the number of principal components to keep in the analysis. We look at the shape and try to find the PC axes that account for most of the variance. Visually, we can see this as the change in slope.

In this case, we can see a big drop from 1 to 2 and then a much slower decline from 2 through 4. This is a good indication that the first eigenvector (PC1) captures most of the variation in Flowering Time. The above table tells us that it's more than half (Proportion of Vairance = 68%).

> Run the above analysis for FVeg and compare the outputs. Which PC axes should we retain?

# Eigenvectors & Loadings

Each principal component axis is just a vector that is calculated by a linear transformation of the input features. You should now have a clear idea of how we can define a linear model to predict `Y` from one or more `X` predictors, with an overall intercept and individual slopes/means for each `X`. The eigenvector is similar except that there is no intercept. Instead of a measured `Y` variable, we have four PC axes, each one a different linear combination of the original Flwr07-Flwr10 columns. If we take these four coefficients and put them into a new vector, we get something called the **eigenvector**. In PCA lingo, the coefficients are called **loadings** and the **eigenvector** is just a vector of loadings.

```{r}
FlwrPCA$loadings
```

The top part of the output shows the loadings for each eigenvector. You can see that that for PC1, all four measurements have similar and positive loadings. For PC2, Flwr07 has a strong positive loading while the other 3 have weak negative loadings.

Think of eigenvector loadings as a measure of how much a particular measurement 'loads' onto a given PC axis. Measurements with higher magnitude have a stronger influence on the PC and the sign gives the direction.

Compare the loadings of PC2 for Flwr07 and Flwr08 above, and compare to plotted graphs of the actual data:

```{r}
qplot(x=FlwrPCA$scores[,2], y=Flwr$Flwr07)
qplot(x=FlwrPCA$scores[,2], y=Flwr$Flwr08)
```

```{r}
cor(FlwrPCA$scores[,2], Flwr$Flwr07)
cor(FlwrPCA$scores[,2], Flwr$Flwr08)
```

Note this weird line in the middle corresponding to the missing values that we replaced with the mean at the beginning.

> Now do the same for PC1. How do those graphs compare to these ones?

We can extract the first column of coefficients as the first 4 elements:

```{r}
Load<-FlwrPCA$loadings[1:4]
print(Load)
```

If we multiply each loading by its corresponding feature vector, and then sum, we get the PC axis, which we can confirm by plotting. We also have to make sure we scale each measurement to z-scores. The `scale` function in R is good for this:

```{r}
testDat<-Flwr %>%
  mutate(PCcalc=scale(Flwr07)*Load[1] + 
           scale(Flwr08)*Load[2] +
           scale(Flwr09)*Load[3] +
           scale(Flwr10)*Load[4])
testDat$Comp.1<-FlwrPCA$scores[,1]
qplot(Comp.1,PCcalc,data=testDat)
```

Take a second to review and make sure you understand these. Before proceeding, you whould be able to define the following (verbally and mathematically):

  * Eigenvectors
  * Eigenvalues
  * Loadings
  * Principal Components Axes
  
# Biological 'Traits'

We can also see some degree of correlation between FlwrPC and VegPC. This is because we ran separate PC analyses for the two sets of traits. We might think this makes sense biologically because the vegetative growth of a plant serves a different function than the reproductive timing.

However, there are other cases where we may want to be more agnostic about traits. In this case, we have 6 populations with different genes for both flowering time and size. So an alternative approach would be to put all the vegetative and reproductive measurements into a single PCA -- again, making sure we use the `cor=T` to account for the fact that different measurements in different years will have different means and variances.

Let's do a new PCA by using all of the measurements, across all of the years. 

For the PCA we only want the measurement columns, AND we want to avoid the 2007 measurements because plants there are a lot of missing data for the 2007 measurements in the Timmins site. We also want to keep Site and Pop for later analyses

```{r}
names(LythrumDat)
PCDat<-LythrumDat[,c(2,6,12:23)]
```

As a shortcut we'll delete rows with missing data using the `na.omit` function with `dplyr`. This will lose a lot of data so some kind of imputation of missing values would be better, but this is okay for demonstration purposes.  

```{r}
PCDat<-PCDat %>%
  na.omit
names(PCDat)
```

Remember to exclude the first two rows since Site and Pop are note measurements that we want to include in the PCA

```{r}
PCfull<-princomp(PCDat[,3:14],cor=T)
summary(PCfull)
loadings(PCfull)
```

We could do a Scree Plot but we'll just retain the first 2 PCs for visualizing. Remember that PC axes are just rescaled versions of the input data, so we can treat these as new variables, even adding them back to our dataset:

```{r}
PCDat$PC1<-PCfull$scores[,1]
PCDat$PC2<-PCfull$scores[,2]
```

Now we can just plot our two PC axes and colour code based on common garden location:

```{r}
qplot(x=PC1,y=PC2,colour=Site,data=PCDat)
```

We can see that there is more overlap between the red and green and less overlap with the blue if we look along the PC2 axis. This represents environmental differences (e.g. plasticity) in the phenotypic measurements that we put into our PCA. 

So what explains the variation along PC1? Let's try plotting those separately and then colour-coding by population

```{r}
qplot(x=PC1,y=PC2,colour=Pop,facets="Site",data=PCDat)
```

With a separate graph from each site, we can clearly see separation of the different genetic populations. This represents population-level genetic differentiation for the traits that load onto PC1.

There are a lot of other things we could explore here. For example, we could use PC1 or PC2 as a predictor or response variable in a linear model. 

```{r}
Mod<-lm(PC1 ~ Pop*Site, data=PCDat)
summary(Mod)
anova(Mod)
```

Both Site and Population affect PC1, and these factors alone explain almost 70% of the variation in PC1.

# Projection

You may remember back to high school math or physics that a **vector** is defined by a **direction** and **magnitude**. We have already seen that the **eigenvalue** is the magnitude of an eigenvector and represents the amount of variation caputured by a principal component axis.

So what is the direction of an eigenvector? It's a direction in multivariate space, with dimesions equal to the number of input features. In the examples so far, we have seen 4-D space for each of Flwr and FVeg, corresponding to four years of data.

Of course, more than 2 dimensions can't be visualized on a computer screen or report, and more than 3 dimensions is a completely foreign concept to the human brain. But we can still visualize multiple dimensions by using **projection**.

Think about what happens when you watch a movie on a 2-dimensional screen. Let's say a scene takes place in a room with a table and some chairs. That's a completely flat image, yet you are able to get a sense for the third dimension. In fact, your visualization of the real world is really just a 2-dimensional field of cells in your retina. You are intuitively able to reconstruct 3D space based on the angles of the lines of the room, table and chairs.

This is called **projection**. The 3D space is **projected** onto 2D space. We can do the same thing for higher dimensions of a PCA. Rather than do these calculations by hand, we can use the `autoplot` function from the `ggfortify` package

```{r}
library(ggfortify)
autoplot(PCfull)
```

We can also use the original dataset to color-code the data.

```{r}
autoplot(PCfull, data=PCDat, colour="Site")
```

And use `loadings=TRUE` to project the eigenvectors to see which measurements contribute most to PC1 vs PC2

```{r}
autoplot(PCfull, data=PCDat, colour="Site", loadings=T, loadings.label=T)
```

Compare the direction of these projected eigenvectors to the loadings in the output for PCfull, above. We can see that PC1 is affected by all three measurements whereas component 2 is more affected by InfMass and Flwr. Notice how the same measurement in different years all have very similar vectors. This shows that the same measurements in different years are collinear.


# QA/QC

PCA has important assumptions that should be checked. As with linear models, we may want to transform some of the input variables to help meet model assumptions. The key assumptions is that input variables are **multivariat normal.** This means that if we graph each pair of input variables, they should form a 'shotgun' pattern that is approximately circular (or oval) with the highest density of points in the middle. As Principal Components are linear combinations of input variables, outliers and non-normal distributions tend to bias the loadings of particular PC axes.

Another important assumption is that the major axes of variation are the axes of interest. In the examples above, we saw how variation along PC1 is largely due to genetic variation while PC2 is largely due to environmental variation. But it may not always be like this. For example, we may have multiple axes of environmental variation and genetic variation might be spread across many axes. In such a case, we may want to rescale or 'rotate' our axes in a way that maximizes variation among populations. Although we can't do this with PCA, there are other methods based on PCA that we can try.

# Beyond PCA

PCA forms the basis for a number of additional analyses in machine learning, so it is worth spending some time to review and understand the concepts in this tutorial. Here are a few examples of PCA-based analyses. 

  * **Discriminant Analysis** -- This is a **supervised learning** approach based on PCA. The key difference is that the eigenvectors are rescaled in a way that maximizes their ability to discriminate between the different groups of the response variable.
  * **Factor Analysis** -- FA builds on PCA by adding unobserved variables called factors. For example, we may have gene expression data from a set of particular pathways, and we want to use factor analysis to look at the behaviour of the smaller subset of pathways rather than the many genes themselves.
  * **Correspondence Analysis** -- Think of this as 2 separate PCAs that you then compare to figure out how one set of data affects the other. A good example of this is community ecology where one set of data is the species community and the other set includes a bunch of environmental variables.



